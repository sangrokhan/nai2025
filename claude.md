# Cellular Network Optimization - Hierarchical Model

## Project Overview
ì…€ë£°ëŸ¬ ë„¤íŠ¸ì›Œí¬ì˜ throughputì„ ì˜ˆì¸¡í•˜ê³  ìµœì í™”í•˜ê¸° ìœ„í•œ ê³„ì¸µì  ë”¥ëŸ¬ë‹ ëª¨ë¸ì…ë‹ˆë‹¤.
- **Physical Layer Encoder**: CQI, SINR, RI ì²˜ë¦¬
- **Link Adaptation Encoder**: MCS í†µê³„ ì²˜ë¦¬
- **Auxiliary Tasks**: ê° ê³„ì¸µì˜ í‘œí˜„ í•™ìŠµ í’ˆì§ˆ ê²€ì¦
- **Main Task**: Throughput ì˜ˆì¸¡

## Project Structure
```
project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ physical_encoder.py      # Physical Layer Encoder
â”‚   â”‚   â”œâ”€â”€ la_encoder.py             # Link Adaptation Encoder
â”‚   â”‚   â”œâ”€â”€ auxiliary_tasks.py        # Auxiliary task modules
â”‚   â”‚   â””â”€â”€ hierarchical_model.py     # í†µí•© ëª¨ë¸
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py           # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”‚   â””â”€â”€ dataset.py                # PyTorch Dataset
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ losses.py                 # Loss í•¨ìˆ˜ë“¤
â”‚   â”‚   â”œâ”€â”€ trainer.py                # í•™ìŠµ ë£¨í”„
â”‚   â”‚   â””â”€â”€ logger.py                 # TensorBoard ë¡œê±°
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ analyzer.py               # ëª¨ë¸ ë¶„ì„
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ early_stopping.py         # Early stopping
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_models.py                # ëª¨ë¸ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_data.py                  # ë°ì´í„° í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_training.py              # í•™ìŠµ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_integration.py           # í†µí•© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ small_model.yaml              # ì‘ì€ ëª¨ë¸ ì„¤ì •
â”‚   â”œâ”€â”€ medium_model.yaml             # ì¤‘ê°„ ëª¨ë¸ ì„¤ì • (ê¸°ë³¸)
â”‚   â””â”€â”€ large_model.yaml              # í° ëª¨ë¸ ì„¤ì •
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                      # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ evaluate.py                   # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ analyze.py                    # ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ data/                             # ë°ì´í„° ë””ë ‰í† ë¦¬
â”œâ”€â”€ checkpoints/                      # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
â”œâ”€â”€ runs/                             # TensorBoard ë¡œê·¸
â”œâ”€â”€ analysis_output/                  # ë¶„ì„ ê²°ê³¼
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## Key Implementation Details

### 1. Data Processing
- **Input Features**:
  - Physical Layer: CQI (16Ã—4), SINR (20Ã—2), RI (4) â†’ ì´ 116 features
  - Link Adaptation: MCS (32Ã—8) â†’ ì´ 256 features
  - Context: UE count, PRB utilization
- **Preprocessing**:
  - log1p transformation
  - Within-group normalization
  - ê° feature groupë³„ ë…ë¦½ ì²˜ë¦¬

### 2. Model Architecture

#### Physical Layer Encoder
```
Input (116 features)
â†’ Feature Encoders (7ê°œ: CQIÃ—4, SINRÃ—2, RIÃ—1)
â†’ Transformer (multi-head attention)
â†’ h_channel (128-dim)
```

#### Link Adaptation Encoder
```
Input (256 MCS features + h_channel)
â†’ MCS Encoders (8ê°œ: 4 layers Ã— 2 MIMO types)
â†’ Channel-aware modulation (FiLM)
â†’ Transformer integration
â†’ h_LA (128-dim)
```

#### Throughput Predictor
```
[h_channel, h_LA, ue_count, prb_util]
â†’ Deep MLP (3-4 layers)
â†’ Throughput prediction (1-dim)
```

### 3. Loss Functions

#### Main Loss
- Throughput MSE (log space)
- Throughput MAE (original space)

#### Physical Auxiliary Losses
- Spectral Efficiency prediction
- RI distribution prediction (KL divergence)
- Channel quality prediction

#### LA Auxiliary Losses
- MCS average prediction
- SU/MU MIMO ratio prediction

**Total Loss**:
```
L_total = L_throughput + Î±Â·L_physical_aux + Î²Â·L_la_aux
```

### 4. Training Configuration

**Medium Model (Recommended)**:
- `channel_dim`: 128
- `num_transformer_layers`: 2
- `num_attention_heads`: 8
- `dropout`: 0.15
- `weight_decay`: 5e-5
- `learning_rate`: 5e-4 â†’ 1e-4 (with scheduling)

**Current Performance**:
- RÂ²: 0.7713
- MAPE: 10.90%
- Train/Val gap: ~11% (ì •ìƒ ë²”ìœ„)

## Implementation Tasks

### Phase 1: Core Implementation (Priority: High)

#### Task 1.1: Data Module
```python
# src/data/preprocessor.py
class CellularDataPreprocessor:
    """
    ì…€ë£°ëŸ¬ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ì „ì²˜ë¦¬

    Requirements:
    - Physical layer groups: CQI (Ã—4), SINR (Ã—2), RI
    - LA layer groups: MCS (4 layers Ã— 2 MIMO types Ã— 32 indices)
    - log1p + normalization
    """

# src/data/dataset.py
class HierarchicalDataset(Dataset):
    """
    PyTorch Dataset

    Requirements:
    - Physical features ì „ì²˜ë¦¬
    - LA features ì „ì²˜ë¦¬
    - Target (throughput) log transformation
    - Context features (ue_count, prb_util)
    """
```

**MCS Data Access Pattern (ì¤‘ìš”!)**:
```python
# MCS ë°ì´í„°ëŠ” ë‹¤ìŒ naming convention ì‚¬ìš©:
for layer in ['ONE_LAYER', 'TWO_LAYER', 'THREE_LAYER', 'FOUR_LAYER']:
    for mimo_type in ['SU_MIMO', 'MU_MIMO']:
        # ì‹¤ì œ ì»¬ëŸ¼ëª…: f'MCS_{layer}_{mimo_type}_MCS{i}' (i=0~31)
        # Dataset key: f'mcs_{layer.lower()}_{mimo_type.lower()}'

# ì˜ˆì‹œ:
# - 'mcs_one_layer_su_mimo': [B, 32]
# - 'mcs_two_layer_mu_mimo': [B, 32]
```

#### Task 1.2: Model Modules
```python
# src/models/physical_encoder.py
class PhysicalLayerEncoder(nn.Module):
    """
    Physical Layer â†’ h_channel

    Components:
    - 7 feature encoders (CQIÃ—4, SINRÃ—2, RIÃ—1)
    - Multi-head attention (Transformer)
    - Aggregation MLP

    Output: h_channel [B, channel_dim]
    """

# src/models/la_encoder.py
class LinkAdaptationEncoder(nn.Module):
    """
    MCS + h_channel â†’ h_LA

    Components:
    - 8 MCS encoders (4 layers Ã— 2 MIMO)
    - Channel-aware modulation (FiLM)
    - Transformer integration
    - Fusion with h_channel

    Output: h_LA [B, channel_dim]
    """

# src/models/auxiliary_tasks.py
class AuxiliaryTasks(nn.Module):
    """Physical layer auxiliary tasks"""

class LAAuxiliaryTasks(nn.Module):
    """LA layer auxiliary tasks"""

# src/models/hierarchical_model.py
class HierarchicalModel(nn.Module):
    """
    í†µí•© ëª¨ë¸

    Forward:
    1. h_channel = physical_encoder(batch)
    2. h_LA = la_encoder(batch, h_channel)
    3. auxiliary tasks on both h_channel and h_LA
    4. throughput_pred from [h_channel, h_LA, context]
    """
```

#### Task 1.3: Loss Functions
```python
# src/training/losses.py

def compute_auxiliary_losses(aux_outputs, batch, weights) -> Dict:
    """
    Physical auxiliary losses

    1. SE prediction: from SINR average
    2. RI distribution: KL divergence
    3. Channel quality: from CQI weighted average
    """

def compute_la_auxiliary_losses(la_aux_outputs, batch, weights) -> Dict:
    """
    LA auxiliary losses

    1. MCS average: weighted average across all MCS stats
    2. SU ratio: SU_total / (SU_total + MU_total)
    """

class HierarchicalLoss(nn.Module):
    """
    Total loss = main + physical_aux + la_aux

    IMPORTANT: Auxiliary lossesê°€ 0ì´ ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜!
    - ì œëŒ€ë¡œ ê³„ì‚°ë˜ëŠ”ì§€ í™•ì¸
    - loss dictì— ì œëŒ€ë¡œ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
    """
```

#### Task 1.4: Training Loop
```python
# src/training/trainer.py
class Trainer:
    """
    í•™ìŠµ ë£¨í”„

    Features:
    - TensorBoard logging
    - Early stopping
    - Learning rate scheduling
    - Gradient clipping
    - Debug mode (ì²« epochì— ìƒì„¸ ì¶œë ¥)

    Logging:
    - Train/Val for all loss components
    - RÂ² and MAPE metrics
    - Learning rate
    - Model gradients & weights (optional)
    """

# src/training/logger.py
class TensorBoardLogger:
    """
    TensorBoard ë¡œê±°

    Log:
    - Scalars: losses, metrics, lr
    - Histograms: gradients, weights
    - Embeddings: h_channel, h_LA
    """
```

### Phase 2: Testing (Priority: High)

#### Test 1: Data Pipeline
```python
# tests/test_data.py

def test_preprocessor():
    """
    Preprocessor í…ŒìŠ¤íŠ¸
    - Physical groups ì •í™•íˆ ì „ì²˜ë¦¬ë˜ëŠ”ì§€
    - LA groups (MCS) ì •í™•íˆ ì „ì²˜ë¦¬ë˜ëŠ”ì§€
    - Normalization ì˜¬ë°”ë¥¸ì§€
    """

def test_dataset():
    """
    Dataset í…ŒìŠ¤íŠ¸
    - ì˜¬ë°”ë¥¸ shape ë°˜í™˜í•˜ëŠ”ì§€
    - ëª¨ë“  keyê°€ ì¡´ì¬í•˜ëŠ”ì§€
    - Batch collation ë™ì‘í•˜ëŠ”ì§€
    """

def test_dataloader():
    """
    DataLoader í…ŒìŠ¤íŠ¸
    - ë°°ì¹˜ ìƒì„± ê°€ëŠ¥í•œì§€
    - ì—¬ëŸ¬ workerì—ì„œ ë™ì‘í•˜ëŠ”ì§€
    """
```

#### Test 2: Model Components
```python
# tests/test_models.py

def test_physical_encoder():
    """
    Physical encoder í…ŒìŠ¤íŠ¸
    - Forward pass ì„±ê³µí•˜ëŠ”ì§€
    - Output shape ë§ëŠ”ì§€
    - Gradient flow ë˜ëŠ”ì§€
    """

def test_la_encoder():
    """
    LA encoder í…ŒìŠ¤íŠ¸
    - h_channel conditioning ì‘ë™í•˜ëŠ”ì§€
    - MCS ëª¨ë“  key ì²˜ë¦¬í•˜ëŠ”ì§€
    - Output shape ë§ëŠ”ì§€
    """

def test_auxiliary_tasks():
    """
    Auxiliary tasks í…ŒìŠ¤íŠ¸
    - ëª¨ë“  prediction head ì‘ë™í•˜ëŠ”ì§€
    - Output range ì ì ˆí•œì§€ ([0,1] ë“±)
    """

def test_hierarchical_model():
    """
    í†µí•© ëª¨ë¸ í…ŒìŠ¤íŠ¸
    - End-to-end forward pass
    - ëª¨ë“  ì¶œë ¥ ìƒì„±ë˜ëŠ”ì§€
    - Parameter count ì˜ˆìƒ ë²”ìœ„ì¸ì§€
    """
```

#### Test 3: Loss Functions
```python
# tests/test_training.py

def test_auxiliary_loss_computation():
    """
    Auxiliary loss ê³„ì‚° í…ŒìŠ¤íŠ¸

    CRITICAL: ì´ê²Œ ì œì¼ ì¤‘ìš”!
    - Physical aux lossê°€ 0ì´ ì•„ë‹Œì§€
    - LA aux lossê°€ 0ì´ ì•„ë‹Œì§€
    - ê° componentë³„ loss ê°’ í™•ì¸
    """

def test_hierarchical_loss():
    """
    Total loss ê³„ì‚° í…ŒìŠ¤íŠ¸
    - ëª¨ë“  componentê°€ í¬í•¨ë˜ëŠ”ì§€
    - Weight ì ìš© ì˜¬ë°”ë¥¸ì§€
    - Backward ë™ì‘í•˜ëŠ”ì§€
    """

def test_training_step():
    """
    í•œ ìŠ¤í… í•™ìŠµ í…ŒìŠ¤íŠ¸
    - Forward â†’ Loss â†’ Backward â†’ Update
    - Loss ê°ì†Œí•˜ëŠ”ì§€
    - Gradient ì¡´ì¬í•˜ëŠ”ì§€
    """
```

#### Test 4: Integration
```python
# tests/test_integration.py

def test_full_training_loop():
    """
    ì†Œê·œëª¨ ë°ì´í„°ë¡œ ì „ì²´ í•™ìŠµ í…ŒìŠ¤íŠ¸
    - 2-3 epoch í•™ìŠµ
    - Loss ê°ì†Œí•˜ëŠ”ì§€
    - Checkpoint ì €ì¥ë˜ëŠ”ì§€
    - TensorBoard ë¡œê·¸ ìƒì„±ë˜ëŠ”ì§€
    """

def test_overfitting_small_batch():
    """
    Overfitting í…ŒìŠ¤íŠ¸ (ëª¨ë¸ capacity í™•ì¸)
    - ì‘ì€ ë°°ì¹˜ (32 samples)
    - Train loss â†’ 0ì— ê°€ê¹Œì›Œì§€ëŠ”ì§€
    - ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì™¸ìš¸ ìˆ˜ ìˆëŠ”ì§€
    """
```

### Phase 3: Analysis Tools (Priority: Medium)

```python
# src/analysis/analyzer.py
class HierarchicalModelAnalyzer:
    """
    ëª¨ë¸ ë¶„ì„ ë„êµ¬

    Features:
    1. Representation extraction (h_channel, h_LA)
    2. Clustering analysis
    3. Correlation with throughput
    4. Auxiliary task performance
    5. Prediction quality metrics
    6. Layer contribution analysis

    Outputs:
    - PNG plots (5-6ê°œ)
    - Summary statistics
    """
```

### Phase 4: Utilities (Priority: Low)

```python
# src/utils/early_stopping.py
class EarlyStopping:
    """Early stopping with patience"""

# src/utils/config.py
def load_config(yaml_path: str) -> Dict:
    """YAML config loader"""

# src/utils/metrics.py
def compute_metrics(y_true, y_pred) -> Dict:
    """RÂ², MAPE, MAE, RMSE ê³„ì‚°"""
```

## Configuration Files

### configs/medium_model.yaml
```yaml
# ê¸°ë³¸ ì„¤ì • (ì¶”ì²œ)
model:
  channel_dim: 128
  num_transformer_layers: 2
  num_attention_heads: 8
  predictor_hidden_dim: 256
  predictor_num_layers: 3
  dropout: 0.15

training:
  batch_size: 128
  num_epochs: 100
  learning_rate: 5.0e-4
  weight_decay: 5.0e-5
  gradient_clip_norm: 1.0

loss:
  main_weight: 1.0
  physical_aux_weights:
    se: 0.25
    ri_dist: 0.15
    quality: 0.15
  la_aux_weights:
    mcs_avg: 0.15
    su_ratio: 0.08

scheduler:
  type: ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 5
  min_lr: 1.0e-6

early_stopping:
  patience: 15
  min_delta: 1.0e-4

paths:
  train_data: ./data/train.parquet
  val_data: ./data/val.parquet
  save_dir: ./checkpoints
  log_dir: ./runs/medium_model
```

## Execution Scripts

### scripts/train.py
```python
"""
Main training script

Usage:
    python scripts/train.py --config configs/medium_model.yaml
    python scripts/train.py --config configs/medium_model.yaml --debug
"""

def main(config_path: str, debug: bool = False):
    # 1. Load config
    # 2. Setup data
    # 3. Create model
    # 4. Setup training
    # 5. Train
    # 6. Save results
```

### scripts/evaluate.py
```python
"""
Evaluation script

Usage:
    python scripts/evaluate.py \
        --checkpoint ./checkpoints/best_model.pt \
        --data ./data/test.parquet \
        --output ./evaluation_results.json
"""
```

### scripts/analyze.py
```python
"""
Analysis script

Usage:
    python scripts/analyze.py \
        --checkpoint ./checkpoints/best_model.pt \
        --data ./data/val.parquet \
        --output_dir ./analysis_output
"""
```

## Testing Commands

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/ -v

# íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest tests/test_data.py -v
pytest tests/test_models.py -v
pytest tests/test_training.py::test_auxiliary_loss_computation -v

# Coverageì™€ í•¨ê»˜ ì‹¤í–‰
pytest tests/ --cov=src --cov-report=html

# í†µí•© í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
pytest tests/test_integration.py -v -s
```

## Debug Mode

í•™ìŠµ ì‹œì‘ ì „ debug modeë¡œ ë¬¸ì œ í™•ì¸:

```python
# scripts/train.pyì—ì„œ
if args.debug:
    # 1. ëª¨ë¸ forward pass í…ŒìŠ¤íŠ¸
    # 2. Loss ê³„ì‚° í…ŒìŠ¤íŠ¸
    # 3. ì²« ë°°ì¹˜ ìƒì„¸ ì¶œë ¥
    # 4. Auxiliary loss 0ì´ ì•„ë‹Œì§€ í™•ì¸
```

**Debug checklist**:
- [ ] Forward pass ì„±ê³µ
- [ ] ëª¨ë“  auxiliary outputs ì¡´ì¬
- [ ] Physical aux loss > 0
- [ ] LA aux loss > 0
- [ ] Total loss ì˜¬ë°”ë¥´ê²Œ í•©ì³ì§
- [ ] Backward ì„±ê³µ
- [ ] Gradients ì¡´ì¬

## Expected Performance

### Current (Medium Model, channel_dim=128)
```
Validation:
  RÂ²: 0.7713
  MAPE: 10.90%
  Train Loss: 0.0455
  Val Loss: 0.0658
  Gap: 44.6% (total), 11% (throughput only)

Status: âœ… Production Ready
```

### Target (with improvements)
```
With regularization + augmentation:
  RÂ²: 0.80-0.82
  MAPE: 9-10%
  Gap: <10% (throughput)

With ensemble:
  RÂ²: 0.82-0.85
  MAPE: 8-9%
```

## Critical Implementation Notes

### 1. MCS Data Naming (ë§¤ìš° ì¤‘ìš”!)
```python
# ì‹¤ì œ CSV/Parquet ì»¬ëŸ¼ëª…
MCS_ONE_LAYER_SU_MIMO_MCS0
MCS_ONE_LAYER_SU_MIMO_MCS1
...
MCS_FOUR_LAYER_MU_MIMO_MCS31

# Preprocessorì—ì„œ group ìƒì„± ì‹œ
for layer in ['ONE_LAYER', 'TWO_LAYER', 'THREE_LAYER', 'FOUR_LAYER']:
    for mimo_type in ['SU_MIMO', 'MU_MIMO']:
        group_name = f'mcs_{layer.lower()}_{mimo_type.lower()}'

# Dataset __getitem__ì—ì„œ ë°˜í™˜ ì‹œ
'mcs_one_layer_su_mimo': tensor([32])

# Modelì—ì„œ ì ‘ê·¼ ì‹œ
batch[f'mcs_{layer}_su_mimo']
```

### 2. Auxiliary Lossê°€ 0ì´ ë˜ëŠ” ë¬¸ì œ
**ì›ì¸**:
- Loss í•¨ìˆ˜ê°€ ì œëŒ€ë¡œ í˜¸ì¶œ ì•ˆ ë¨
- Outputs dictì— auxiliary ê²°ê³¼ ì—†ìŒ
- Loss dictì— í•©ì³ì§€ì§€ ì•ŠìŒ

**í•´ê²°**:
- Debug modeë¡œ ì²« ë°°ì¹˜ í™•ì¸
- ëª¨ë“  auxiliary output ì¡´ì¬ í™•ì¸
- Loss ê³„ì‚° ë‹¨ê³„ë³„ print

### 3. Regularization Strategy
```python
# í˜„ì¬ gapì´ ìˆì§€ë§Œ ì„±ëŠ¥ì€ ì¢‹ìŒ
# â†’ Capacity ì¤„ì´ì§€ ë§ê³  regularization ê°•í™”

dropout: 0.15  # 0.1 â†’ 0.15
weight_decay: 5e-5  # 1e-5 â†’ 5e-5
aux_weights: 0.7x  # ì „ì²´ì ìœ¼ë¡œ 30% ê°ì†Œ
```

## Requirements

### Python Packages
```txt
torch>=2.0.0
tensorboard>=2.13.0
numpy>=1.24.0
pandas>=2.0.0
pyarrow>=12.0.0
scikit-learn>=1.3.0
scipy>=1.11.0
matplotlib>=3.7.0
seaborn>=0.12.0
tqdm>=4.65.0
pytest>=7.4.0
pytest-cov>=4.1.0
pyyaml>=6.0
```

### Hardware
- GPU: 8GB+ VRAM (RTX 3070 ì´ìƒ ê¶Œì¥)
- RAM: 16GB+
- Storage: 10GB+ (ë°ì´í„° + ì²´í¬í¬ì¸íŠ¸)

## Getting Started

```bash
# 1. í™˜ê²½ ì„¤ì •
pip install -r requirements.txt

# 2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë°ì´í„° ì—†ì´ë„ ê°€ëŠ¥)
pytest tests/test_models.py -v

# 3. Debug modeë¡œ í•™ìŠµ ì‹œì‘
python scripts/train.py --config configs/medium_model.yaml --debug

# 4. TensorBoard í™•ì¸
tensorboard --logdir=./runs

# 5. ì •ìƒ í•™ìŠµ ì‹œì‘
python scripts/train.py --config configs/medium_model.yaml

# 6. ë¶„ì„ ì‹¤í–‰
python scripts/analyze.py \
    --checkpoint ./checkpoints/best_model.pt \
    --data ./data/val.parquet \
    --output_dir ./analysis_output
```

## Troubleshooting

### Issue 1: Auxiliary Loss = 0
```bash
# Debug mode ì‹¤í–‰
python scripts/train.py --config configs/medium_model.yaml --debug

# í™•ì¸í•  ê²ƒ:
# - "=== DEBUG: Model Outputs ===" ì„¹ì…˜
# - physical_aux, la_aux ì¶œë ¥ ì¡´ì¬í•˜ëŠ”ì§€
# - "=== DEBUG: Loss Values ===" ì„¹ì…˜
# - physical_total_aux, la_total_la_auxê°€ 0ì´ ì•„ë‹Œì§€
```

### Issue 2: Out of Memory
```yaml
# configs/medium_model.yaml ìˆ˜ì •
training:
  batch_size: 64  # 128 â†’ 64
  gradient_accumulation_steps: 2  # ì¶”ê°€
```

### Issue 3: Train/Val Gap Too Large
```yaml
# regularization ê°•í™”
model:
  dropout: 0.20  # 0.15 â†’ 0.20

training:
  weight_decay: 1.0e-4  # 5e-5 â†’ 1e-4
```

## Next Steps

1. **Data augmentation**: ImprovedAugmentation í´ë˜ìŠ¤ êµ¬í˜„
2. **Ensemble**: ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ í‰ê· 
3. **Hyperparameter tuning**: Optuna ì‚¬ìš©
4. **More data**: ë°ì´í„° ì¶”ê°€ ìˆ˜ì§‘ (ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬)

---

## Important Notes for Claude Code

1. **Start with tests**: êµ¬í˜„ ì „ì— í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‘ì„±
2. **Debug mode first**: í•™ìŠµ ì „ì— ë°˜ë“œì‹œ debug modeë¡œ í™•ì¸
3. **Modular design**: ê° componentë¥¼ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•˜ê²Œ
4. **Clear naming**: MCS ë°ì´í„° naming convention ì—„ê²©íˆ ì¤€ìˆ˜
5. **Logging**: ëª¨ë“  ì¤‘ìš”í•œ ê°’ë“¤ì„ TensorBoardì— ë¡œê¹…

**Critical Path**:
1. Data pipeline êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
2. Model components êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
3. Loss functions êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸ (auxiliary loss 0 ì•„ë‹Œì§€ í™•ì¸!)
4. Training loop êµ¬í˜„
5. Debug modeë¡œ ì „ì²´ flow í™•ì¸
6. ì‹¤ì œ í•™ìŠµ ì‹œì‘

Good luck! ğŸš€
