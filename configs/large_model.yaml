# Large Model Configuration
# Maximum performance, requires more GPU memory

model:
  channel_dim: 256
  num_transformer_layers: 3
  num_attention_heads: 16
  predictor_hidden_dim: 512
  predictor_num_layers: 4
  dropout: 0.2

training:
  batch_size: 64  # Smaller batch due to memory
  num_epochs: 150
  learning_rate: 3.0e-4
  weight_decay: 1.0e-4
  gradient_clip_norm: 1.0

loss:
  main_weight: 1.0
  physical_aux_weights:
    se: 0.3
    ri_dist: 0.2
    quality: 0.2
  la_aux_weights:
    mcs_avg: 0.2
    su_ratio: 0.1

scheduler:
  type: ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 7
  min_lr: 1.0e-6

early_stopping:
  patience: 20
  min_delta: 1.0e-4

paths:
  train_data: ./data/train.parquet
  val_data: ./data/val.parquet
  save_dir: ./checkpoints/large_model
  log_dir: ./runs/large_model
